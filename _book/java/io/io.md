# IO

* ## 文件描述符fd

lunix系统把任何对象看做是文件，文件就是一串二进制流，对数据\(流\)的读写操作就是对文件的操作，所以当我们的进程在做读写操作时会返回一个记录访问位置的索引值,当我们继续操作该文件时可直接通过这个索引值到达上一次的位置。

文件描述符\(file description\)，用于表述指向文件引用的抽象，文件描述符在形式上是一个非负整数,实际上它是一个索引值,指向内核为每一个进程所维护的该进程打开文件的记录表，当程序打开一个现有文件或者创建一个新文件时，内核就向该进程返回一个文件描述符。

* ## 用户空间和内核空间与进程

现在操作系统都是采用虚拟存储器,对于32位操作系统,它的寻址空间为4G.操作系统的核心是内核,独立于普通程序,可以访问受保护的内存空间,也有访问底层硬件设备的所有权限.为了保证用户进程不能直接操作内核,保证内核安全,操作系统将虚拟空间划分为两部分,一部分称为内核空间,一部分称为用户空间,这两部分空间大小和地址范围分别为1G和3G,内核空间供内核使用,用户空间供用户进程使用。

进程是程序的一次动态执行过程,它经历了从代码加载,执行到执行完毕的一个完整过程,这个过程也是进程本身从产生,发展到最终消亡的过程,多进程操作系统能同时运行多个进程,由于CPU具备分时机制,所以每个进程都能获得自己的时间片,由于CPU执行速度非常快,使得所有程序好像是在同时运行一样。

在操作系统中进程是进行系统资源分配,调度和管理的最小单位,进程在执行过程中拥有独立的内存单元,当操作系统加载程序到内存中,操作系统会为每个进程分配4G的虚拟内存空间.地址从0x00000000到0xFFFFFFFF,其中1G\(3-4\)是内核所使用的内核空间,3G\(0-3\)是进程使用的用户空间。

这里要注意的是系统为每个进程分配4G的虚拟内存空间,实际上这4G的虚拟内存是一个可寻址的地址范围,并不是实际的物理内存,这个可寻址的4G地址范围由内存区域表来管理.每个进程所用到的内存区域会通过页表映射到物理内存,所以每个进程都可以使用同样的虚拟内存地址而不冲突,他们的实际物理地址是不同的。

为了控制进程的执行,内核必须有能力挂起正在CPU上运行的进程,并恢复以前挂起的某个进程的执行.这种行为被称为进程切换,任务切换或上下文切换,尽管每个进程都有自己的地址空间,但所有进程都在同一个CPU寄存器里,因此,在恢复一个进程执行前,内核必须确保每个寄存器中含有挂起进程时所需要的值.进程恢复执行前必须装入寄存器的一组数据,称为硬件上下文\(hardware Context\),硬件上下文包含了进程恢复时所需要的所有信息。一个进程的运行转到另一个进程上,需要做很多交接记录位置的动作。

进程的切换和阻塞

进程的切换是内核执行该进程的时间片到期而主动挂起该进程,切换到另一个进程的动作.但是进程在一个时间片内执行过程中,遇到某些期望的事件未发生那么进程就会放弃处理机成了阻塞,致使进程阻塞的典型事件有：请求I/O，申请缓冲空间等。

缓存IO

缓冲区以及对缓冲区的操作,是所有IO的基础,进程执行IO操作可以简单的描述为缓冲区的数据读与写。  
在数据read传输过程中需要将数据从磁盘中拷贝到内核空间的的缓冲区,然后从内核空间拷贝到进程用户空间,这个过程会经历两个阶段:1,等待数据准备,2,将数据重内核拷贝到用户空间,而且这两个过程是需要时间的,这就造成了阻塞.称为阻塞IO。在这个基础上,为了充分利用CPU资源,发展出了非阻塞IO,IO多路复用,信号驱动IO,异步IO。形成了Linux中五种IO类型。其中前四种都是同步IO。

* ## Linux中的IO基本流程

## ![](images/io-in-linux.jpg)

* ## 阻塞I/O（blocking I/O）

## ![](images/io-blocked.jpg)

当用户进程调用了recfrom这个系统调用,系统内核就开始了IO的第一个阶段,准备数据阶段\(对于网络IO,很多时候数据在一开始还没有到达,没有接收到一个完整的UDP包,这个时候内核就要等待足够的数据到来,磁盘IO的情况就是等待磁盘数据从磁盘上读取到内核空间\),这个过程需要等待,而用户进程这边整个进程就会被阻塞 ,当内核空间把数据准备好了,返回给用户进程一个结果,用户进程才解除阻塞状态。

* ## 非阻塞I/O （nonblocking I/O）

## ![](images/io-none-blocked.jpg)

非阻塞IO是对阻塞IO的一个改进,即在内核未完成准备数据的时候,返回一个状态error告诉进程我没准备好,用户进程收到error状态会继续发起发起IO请求,直到内核空间准备好了数据,返回正确的状态。

* ## I/O复用\(select 和poll\) （I/O multiplexing）

## ![](images/io-multiplexing.jpg)

linuxIO多路复用技术提供一个单进程,单线程内监听多个IO读写时间的机制,其基本原理是各个IO将句柄设置为非阻塞IO,然后将各个IO句柄注册到linux提供的IO复用函数上\(select,poll或者epoll\),如果某个句柄的IO数据就绪,则函数返回,由于开发者进行该IO数据处理.多路复用函数帮我们进行了多个非阻塞IO数据是否就绪的轮询操作,只不过IO多路复用函数的轮询更有效率,因为函数一次性传递文件描述符到内核态,在内核态中进行轮询\(epoll则是进行等待边缘事件的触发\),不必反复进行用户态和内核态的切换。linuxIO的多路复用技术主要的实现方式,select,poll,和epoll,过根据触发方式不同,与是否需要轮询的的不同。

1. **SELECT:**  
   select是Linux最早支持的多路IO复用函数，其函数原型为：  
   int select\(int nfds, fd\_set\* readfds, fd\_set\* writefds, fd\_set\* errorfds, struct timeval\* timeout\);

   ```
       参数nfds是所有文件描述符的数量+1，而readfds、writefds和errorfds分别为等待读、写和错误IO操作的文件描述符的集合，而timeout是超时时间，超过timeout时间select将返回（0表示不阻塞，NULL则是没有超时时间）。

       select的返回值是有可用的IO操作的文件描述符数量，如果超时返回0，如果发生错误返回-1。

       select函数需要和四个宏配合使用：FD\_SET\(\)、FD\_CLR\(\)、FD\_ZERO\(\)和FD\_ISSET\(\)。具体使用不再介绍，可以参考资料\[7,8\]的相关内容，下面介绍select函数的内部实现原理和主要流程：
   ```

   1、使用copy\_from\_user从用户空间拷贝fd\_set到内核空间；

   2、遍历所有fd，调用其对应的poll函数，再由poll函数调用\_\_pollwait函数；

   3、poll函数会判断当前文件描述符上的IO操作是否就绪，并利用\_\_pollwait的主要工作就是把当前进程挂到设备的等待队列中，但这并不代表进程会睡眠；

   4、poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd\_set赋值；

   5、如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule\_timeout使进程进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程，更新fd\_set后select返回；

   6、如果超过超时时间schedule\_timeout，还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd，流程如上；

   7、把fd\_set从内核空间拷贝到用户空间，select返回。

```
        从上面的select内部流程中可以看出，select操作既有阻塞等待，也有主动轮询，相比于纯粹的轮询操作，效率应该稍微高一些。但是其缺点仍然十分明显：
```

1、每次调用select，都需要把fd集合从用户态拷贝到内核态返回时还要从内核态拷贝到用户态，这个开销在fd很多时会很大；

2、每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大；

3、select返回后，用户不得不自己再遍历一遍fd集合，以找到哪些fd的IO操作可用；

4、再次调用select时，fd数组需要重新被初始化；

5、select支持的文件描述符数量太小了，默认是1024。

1. **POLL**  
   poll的函数原型为int poll\(struct pollfd \*fds, nfds\_t nfds, int timeout\)。其实现和select非常相似，只是描述fd集合的方式不同，poll通过一个pollfd数组向内核传递需要关注的事件，故没有描述符个数的限制。

   ```
       pollfd中的events字段和revents分别用于标示关注的事件和发生的事件，故pollfd数组只需要被初始化一次。

       poll的实现机制与select类似，其对应内核中的sys\_poll，只不过poll向内核传递pollfd数组，然后对pollfd中的每个描述符进行poll。

       poll返回后，同样需要对pollfd中的每个元素检查其revents值，来得指事件是否发生。

       由此可见，poll除了没有文件描述个数限制和文件描述符数组只需初始化一次以外，select的其他缺点扔存在，而存在的缺点是select和poll性能低的主要原因。
   ```

2. **EPOLL**  
    Epoll是Linux 2.6版本之后才引入的一种新的多路IO复用技术，epoll解决了select技术的所有主要缺点，可以取代select方式成为推荐的多路IO复用技术。

   ```
       epoll通过epoll\_create创建一个用于epoll轮询的描述符，通过epoll\_ctl添加/修改/删除事件，通过epoll\_wait等待IO就绪或者IO状态变化的事件发生，epoll\_wait的第二个参数用于存放结果。

       epoll与select、poll不同，首先，其不用每次调用都向内核拷贝事件描述信息，在第一次调用后，事件信息就会与对应的epoll描述符关联起来。另外epoll不是通过轮询，而是通过在等待的描述符上注册回调函数，当事件发生时，回调函数负责把发生的事件存储在就绪事件链表中，最后写到用户空间。

       epoll返回后，该参数指向的缓冲区中即为发生的事件，对缓冲区中每个元素进行处理即可，而不需要像poll、select那样进行轮询检查。

       之所以epoll能够避免效率低下的主动轮询，而完全采用效率更高的被动等待IO事件通知，是因为epoll在返回时机上支持被成为“边沿触发”（edge=triggered）的新思想，与此相对，select的触发时机被成为“水平触发”（level-triggered）。epoll同时支持这两种触发方式。

       边沿触发是指当有新的IO事件发生时，epoll才唤醒进程之后返回；而水平触发是指只要当前IO满足就绪态的要求，epoll或select就会检查到然后返回，即使在调用之后没有任何新的IO事件发生。

       举例来说，一个管道内收到了数据，注册该管道描述符的epoll返回，但是用户只读取了一部分数据，然后再次调用了epoll。这时，如果是水平触发方式，epoll将立刻返回，因为当前有数据可读，满足IO就绪的要求；但是如果是边沿触发方式，epoll不会返回，因为调用之后还没有新的IO事件发生，直到有新的数据到来，epoll才会返回，用户可以一并读到老的数据和新的数据。

       通过边沿触发方式，epoll可以注册回调函数，等待期望的IO事件发生，系统内核会在事件发生时通知，而不必像水平触发那样去主动轮询检查状态。边沿触发和水平触发方式类似于电子信号中的电位高低变化，由此得名。
   ```

3. ## 信号驱动I/O （signal driven I/O ）

   ## ![](images/io-signal.jpg)

   信号驱动的IO是一种半异步的IO模型。使用信号驱动I/O时，当网络套接字可读后，内核通过发送SIGIO信号通知应用进程，于是应用可以开始读取数据。

   ```
       具体的说，程序首先允许套接字使用信号驱动I/O模式，并且通过sigaction系统调用注册一个SIGIO信号处理程序。当有数据到达后，系统向应用进程交付一个SIGIO信号，然后应用程序调用read函数从内核中读取数据到用户态的数据缓存中。这样应用进程都不会因为尚无数据达到而被阻塞，应用主循环逻辑可以继续执行其他功能，直到收到通知后去读取数据或者处理已经在信号处理程序中读取完毕的数据。

       设置套接字允许信号驱动IO的步骤如下：
   ```

   1.注册SIGIO信号处理程序。\(安装信号处理器\)

   2.使用fcntl的F\_SETOWN命令，设置套接字所有者。（设置套接字的所有者）

   3.使用fcntl的F\_SETFL命令，置O\_ASYNC标志，允许套接字信号驱动I/O。（允许这个套接字进行信号输入输出）

   ```
     之所以说信号驱动的IO是半异步的，是因为实际读取数据到应用进程缓存的工作仍然是由应用自己负责的，而这部分工作执行期  间进程依然是阻塞的，如上图中的后半部分。而在下面介绍的异步IO则是完全的异步。
   ```

   ## 异步I/O （asynchronous I/O）

   ## ![](images/io-aio.jpg)

   异步I/O模型是一种处理与I/O重叠进行的模型。读请求会立即返回，说明read 请求已经成功发起了。在后台完成读操作时，应用程序然后会执行其他处理操作。当read 的响应到达时，就会产生一个信号或执行一个基于线程的回调函数来完成这次I/O 处理过程。

           在一个进程中为了执行多个I/O请求而对计算操作和I/O 处理进行重叠处理的能力利用了处理速度与I/O速度之间的差异。当一个或多个I/O 请求挂起时，CPU可以执行其他任务；或者更为常见的是，在发起其他I/O的同时对已经完成的I/O 进行操作。

           在传统的I/O模型中，有一个使用惟一句柄标识的I/O 通道。在 UNIX® 中，这些句柄是文件描述符（这等同于文件、管道、套接字等等）。在阻塞I/O中，我们发起了一次传输操作，当传输操作完成或发生错误时，系统调用就会返回。

           在异步非阻塞I/O中，我们可以同时发起多个传输操作。这需要每个传输操作都有惟一的上下文，这样我们才能在它们完成时区分到底是哪个传输操作完成了。在AIO中，这是一个aiocb（AIO I/O Control Block）结构。这个结构包含了有关传输的所有信息，包括为数据准备的用户缓冲区。在产生I/O（称为完成）通知时，aiocb结构就被用来惟一标识所完成的I/O操作。  
   以read操作为例，一个异步IO操作的时序流程如上图所示，

           从上图中可以看出，比起信号驱动的IO那种半异步模式，异步IO中从内核拷贝数据到用户缓存空间的工作也是有系统完成的异步过程，用户程序只需要在指定的数组中引用数据即可。

           数据接收后的处理程序是一个回调函数，Linux提供了两种机制实现异步IO的回调函数：

           一种是信号回调函数机制，这种机制跟信号驱动的IO类似，利用信号触发回调函数的执行以处理接收的数据，这回中断正在执行的代码，而不会产生新的进程和线程；

           另一种是线程回调函数机制，在这种机制下也需要编写相同的回调函数，但是这个函数将注册到异步IO的事件回调结构体对象中，当数据接收完成后将创建新的线程，在新的线程中调用回调函数进行数据处理。

   ## **各个IO模型的比较和应用场景**

  
           为了比较各个IO模型的性能，这里设计了三种最主要的应用场景，分别是单个用户连接的频繁IO操作、少量用户连接的并发频繁IO操作、大量用户连接的并发频繁IO操作。在进行性能比较时，主要考虑的是总的IO等待、系统调用情况和CPU调度切换，IO等待越少、系统调用越少、CPU调度切换越少意味着IO操作的高效率。

  




           在单个用户连接频繁的IO操作中，可以采用单线程单进程的方式，这样可以不用考虑进程内部的CPU调度，只需关注IO等待和系统调用的频率。从上面各个IO模型的流程时序图来看，AIO的用户程序在执行Io操作时没有任何Io等待，而且只需要调用IO操作时一次系统调用，由于是异步操作，信号操作的回传不需要进行系统调用，连由内核返回用户态的系统调用都省了，因此效率最高。

           在信号驱动的IO模型中，IO等待时间要比基本的阻塞式IO和多路复用IO要少，只需要等待数据从内核到用户缓存的操作。但是信号驱动的IO模型和多路复用IO的系统调用次数一样，需要两次系统调用，共四次上下文切换，而基本的阻塞模式只需要一次系统调用。在IO频繁的场景下，还是基本阻塞IO效率最高，其次为信号驱动IO，然后是多路复用IO。

  


           基本非阻塞IO的性能最差，因为在IO等待期间不仅不交出CPU控制权，还一遍又一遍进行昂贵的系统调用操作进行主动轮询，而主动轮询对于IO操作和业务操作都没有实际的意义，因此CPU计算资源浪费最严重。



           在单个用户连接的频繁IO操作中，性能排名有好到差为：AIO&gt;基本阻塞IO&gt;信号IO&gt;epoll&gt;poll&gt;select&gt;基本非阻塞IO。

  


           在少量用户下的频繁IO操作中，基本阻塞IO一般要使用多线程操作，因此要产生额外的线程调度工作。虽然由于线程较少，远少于系统的总进程数，但是由于IO操作频繁，CPU切换还是会集中在IO操作的各个线程内。

           对于基本阻塞IO和多路复用IO来讲，虽然多路IO复用一次系统调用可以完成更多的IO操作，但是在IO操作完成后对于每个IO操作还是要系统调用将内核中的数据取回到用户缓存中，因此系统调用次数仍然比阻塞IO略多，但线程切换的开销更大。特别对于select来说，由于select内部采用半轮询方式，效率不如阻塞方式，因此在这种少量用户连接的IO场景下，还不能只通过理论判断基本阻塞IO和select方式孰优孰劣。

           其他的IO模型类似于单用户下，不再分析，由此得出在少量用户连接IO操作下的IO模型性能，由好到坏依次为AIO&gt;信号IO&gt;epoll&gt;基本阻塞IO?poll&gt;select&gt;基本非阻塞IO。

  


           在大量，甚至海量用户的并发频繁IO操作下，多路IO复用技术的性能会全面超越简单的多线程阻塞IO，因为这时大量的CPU切换操作将显著减少CPU效率，而多路复用一次完成大量的IO操作的优势更加明显。对于AIO和信号IO，在这种场景下依然有着更少的IO等待和更少的系统调用操作，性能依然最好。

           由此可见，在大量用户的并发频繁IO操作下，IO性能由好到差依次为AIO&gt;信号IO&gt;epoll&gt;poll&gt;select&gt;基本阻塞IO&gt;基本非阻塞IO。

           需要说明的是，以上三种场景都强调了是IO频繁的，如果是IO不频繁的以上三种场景，各个IO模型的性能表现又如何呢？结果不得而知，但是没必要纠结于此，选择一个最擅长的IO模型编程即可，既然IO不频繁，这种性能优劣的比较也就没有太大意义了。

4. ## 同步阻塞IO\(BIO\)

### BIO通讯示意图 ![BIO通讯模型图](images/bio0.jpg) 该模型的通讯过程：

* #### 服务端有独立的accept线程负责监听客户端的链接；
* #### Accept线程接收到客户端的链接后开启新的工作线程进行链接处理；
* #### 新的工作线程处理完毕后并响应给客户端，然后线程销毁；

### 该模型下的问题：

* #### 服务端的工作线程数和客户端的连接数1:1；
* #### 当访问量增大，工作线程膨胀，系统内存飙升；
* #### 线程大量的销毁和重建，系统性能急剧下降；
* #### 最终导致进程僵死或系统宕机；
* ## 伪异步IO\(BIO\)

### BIO通讯示意图 ![BIO通讯模型图](images/bio1.jpg) 该模型的改良之处：

* #### 采用了线程池，不在反复的创建销毁线程；
* #### 一定层度上提升了吞吐量和系统效率；

### BIO的同步和阻塞

#### java.io.InputStream中的read方法以及重载都会block。

```
/**
 * Reads the next byte of data from the input stream. The value byte is
 * returned as an <code>int</code> in the range <code>0</code> to
 * <code>255</code>. If no byte is available because the end of the stream
 * has been reached, the value <code>-1</code> is returned. This method
 * blocks until input data is available, the end of the stream is detected,
 * or an exception is thrown.
 *
 * <p> A subclass must provide an implementation of this method.
 *
 * @return     the next byte of data, or <code>-1</code> if the end of the
 *             stream is reached.
 * @exception  IOException  if an I/O error occurs.
 */
public abstract int read() throws IOException;
```

#### 对于read，除非遇到如下三种情况，其余情况都会阻塞：

* #### 有数据可读
* #### 且可读数据读取完毕
* #### 发生异常

#### 对于write，同样是写入到输出流完毕，或者发生异常之前被阻塞。

#### 这说明当写入一方缓慢将会导致读取的一方将会被阻塞,当读取一方缓慢也会导致写入的阻塞，而网络传输缓慢也是导致双方共同缓慢的原因。在网络抖动时候，TCP中的信令通道将会调整window size，直到为0，此时双方将会处于Keep-Alive状态，发送方不能再向TCP缓冲区写入，写入操作将会无限期阻塞，直到TCPwindow size大于0或者发生异常。 BIO中阻塞的具体含义就是消息（线程）读取或者写入的阻塞，阻塞期间其他消息（线程）将会在队列缓冲区中排队。这个排队指的就是BIO中的同步的具体含义。由于在具体场景中单个消息（线程）操作的阻塞，导致在全局中出现多个消息（线程）同步的现象。

#### BIO模型简单，编程复杂度低，适用于低并发,低负载的应用程序。

## 同步非阻塞IO\(NIO\)

### NIO通讯模型图 !\[NIO通讯模型图\]\(images/xxx.png,'待补充'\)

### NIO关键概念

* Buffer
* Channel
* Selector

## 异步非阻塞IO

参考资料：

[https://mp.weixin.qq.com/s?\_\_biz=MzI4NTEzMjc5Mw==∣=2650554694&idx=1&sn=b923effe8a7feed34f2d6637c4041df9&chksm=f3f833d0c48fbac69c0118c20bb7f8d983e0e571cf7cbf4efffc925c2324533b4d6ca463ac11&scene=21\#wechat\_redirect](https://mp.weixin.qq.com/s?__biz=MzI4NTEzMjc5Mw==&mid=2650554694&idx=1&sn=b923effe8a7feed34f2d6637c4041df9&chksm=f3f833d0c48fbac69c0118c20bb7f8d983e0e571cf7cbf4efffc925c2324533b4d6ca463ac11&scene=21#wechat_redirect)

[https://mp.weixin.qq.com/s?\_\_biz=MzI4NTEzMjc5Mw==∣=2650554708&idx=1&sn=4fa4e599c5028825fda5ead907ec86a6&chksm=f3f833c2c48fbad49fda347833f14f553f764fc0e46ae71073d0b31028f7ec4f85b60d448e9a&mpshare=1&scene=1&srcid=0531oQOJ0j7hUzTSyGyQgRcU&rd2werd=1\#wechat\_redirect](https://mp.weixin.qq.com/s?__biz=MzI4NTEzMjc5Mw==&mid=2650554708&idx=1&sn=4fa4e599c5028825fda5ead907ec86a6&chksm=f3f833c2c48fbad49fda347833f14f553f764fc0e46ae71073d0b31028f7ec4f85b60d448e9a&mpshare=1&scene=1&srcid=0531oQOJ0j7hUzTSyGyQgRcU&rd2werd=1#wechat_redirect)

